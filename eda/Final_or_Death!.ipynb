{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "import seaborn as sns\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 15)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.style.use(\"ggplot\")\n",
    "sns.set(font_scale=1.5)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "# from pygeocoder import Geocoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "std_scaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "mm_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "austin = pd.read_pickle(\"../pickled_data/austin_all.pickled\")\n",
    "sanfran = pd.read_pickle(\"../pickled_data/sf_all_df.pickled\")\n",
    "LA = pd.read_pickle(\"../pickled_data/LA_new_df.pickled\")\n",
    "chicago = pd.read_pickle(\"../pickled_data/chicago_trips_df.pickled\")\n",
    "london = pd.read_pickle(\"../pickled_data/london_counts_df.pickled\")\n",
    "DC = pd.read_pickle(\"../pickled_data/DC_df.pickled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "model = LogisticRegressionCV(solver='lbfgs', multi_class='ovr', cv=5)\n",
    "log_params = {'penalty': ['l1', 'l2'],\n",
    "                   'solver': ['liblinear'],\n",
    "                   'Cs': [np.logspace(3, 0, 10)]}\n",
    "grid_log = GridSearchCV(model, log_params, cv=5, n_jobs=2)\n",
    "pipe = Pipeline(steps=[('scaler', scaler),\n",
    "                       ('model', grid_log)])\n",
    "\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion='gini',\n",
    "                                    max_depth=3, \n",
    "                                    random_state=1)\n",
    "g_params = {'max_depth':list(range(1, 11))+[None],'max_features':[None, 1, 2, 3],'max_leaf_nodes':[None,2,3,4],\n",
    "             'min_samples_leaf':[2,3,4, 5, 10, 15, 20, 25, 30, 40, 50],'min_samples_split':[2,3]}\n",
    "gridsearch = GridSearchCV(classifier , g_params ,return_train_score=True,  cv=10)\n",
    "pipe_class = Pipeline(steps=[('scaler', scaler),\n",
    "                       ('model', gridsearch)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>trip_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-07-12</th>\n",
       "      <td>41.589286</td>\n",
       "      <td>9.675733e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-04</th>\n",
       "      <td>33.869352</td>\n",
       "      <td>8.518074e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-21</th>\n",
       "      <td>23.051230</td>\n",
       "      <td>7.797141e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-29</th>\n",
       "      <td>54.303937</td>\n",
       "      <td>1.123848e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-09</th>\n",
       "      <td>37.780872</td>\n",
       "      <td>1.232019e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-17</th>\n",
       "      <td>41.827688</td>\n",
       "      <td>1.216759e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>31.076132</td>\n",
       "      <td>1.129813e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-09</th>\n",
       "      <td>31.836265</td>\n",
       "      <td>5.711803e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06</th>\n",
       "      <td>13.034602</td>\n",
       "      <td>1.168623e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-22</th>\n",
       "      <td>21.475000</td>\n",
       "      <td>6.422135e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>702 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             duration       trip_id\n",
       "start_time                         \n",
       "2018-07-12  41.589286  9.675733e+07\n",
       "2018-05-04  33.869352  8.518074e+07\n",
       "2018-03-21  23.051230  7.797141e+07\n",
       "2018-12-29  54.303937  1.123848e+08\n",
       "2019-06-09  37.780872  1.232019e+08\n",
       "...               ...           ...\n",
       "2019-05-17  41.827688  1.216759e+08\n",
       "2019-01-07  31.076132  1.129813e+08\n",
       "2017-11-09  31.836265  5.711803e+07\n",
       "2019-03-06  13.034602  1.168623e+08\n",
       "2017-12-22  21.475000  6.422135e+07\n",
       "\n",
       "[702 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUSTIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUSTIN:\n",
      "--------------------------\n",
      "Baseline: 0.3344\n",
      "--------------------------\n",
      "Logistic Regression\n",
      "--------------------------\n",
      "Cross validation 0.6125286609941781\n",
      "train_score: 0.6308571428571429\n",
      "test_score: 0.6026666666666667\n",
      "--------------------------\n",
      "Grid search: Decision Tree Classifier\n",
      "--------------------------\n",
      "Cross validation 0.5931974175250037\n",
      "train_score: 0.664\n",
      "test_score: 0.5706666666666667\n"
     ]
    }
   ],
   "source": [
    "# austin_per_day = austin.groupby(\"dateasdatetime\").mean()\n",
    "# austin_per_day[\"count\"] = austin.dateasdatetime.value_counts()\n",
    "# both work!\n",
    "austin_per_day = austin.groupby([austin[\"start_time\"].dt.date]).mean()\n",
    "austin_per_day[\"count\"] = austin.groupby([austin[\"start_time\"].dt.date]).count()[\"bikeid\"]\n",
    "q1 = austin_per_day['count'].quantile(q=1/3)\n",
    "q2 = austin_per_day['count'].quantile(q=2/3)\n",
    "q3 = austin_per_day['count'].quantile(q=3/3)\n",
    "list_bin = []\n",
    "for val in austin_per_day['count']:\n",
    "    if val < q1:\n",
    "        list_bin.append(0)\n",
    "    elif val>= q1 and val <q2:\n",
    "        list_bin.append(1)\n",
    "    else:\n",
    "        list_bin.append(2)\n",
    "        \n",
    "austin_per_day['count_class'] = list_bin\n",
    "austin_per_day.dropna(inplace=True)\n",
    "del austin_per_day['count']\n",
    "\n",
    "X = austin_per_day.copy()\n",
    "y = X.pop('count_class')\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe_class.fit(X_train, y_train)\n",
    "print('AUSTIN:')\n",
    "print('--------------------------')\n",
    "print('Baseline:',austin_per_day.count_class.value_counts(normalize=True).max())\n",
    "print('--------------------------')\n",
    "print('Logistic Regression')\n",
    "print('--------------------------')\n",
    "print('Cross validation' , cross_val_score(pipe, X_train, y_train, cv=5).mean())\n",
    "print('train_score:',pipe.score(X_train, y_train))\n",
    "print('test_score:',pipe.score(X_test, y_test))\n",
    "print('--------------------------')\n",
    "print('Grid search: Decision Tree Classifier')\n",
    "print('--------------------------')\n",
    "print('Cross validation' , cross_val_score(pipe_class, X_train, y_train, cv=5).mean())\n",
    "print('train_score:',pipe_class.score(X_train, y_train))\n",
    "print('test_score:',pipe_class.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAN FRANCISCO:\n",
      "--------------------------\n",
      "Baseline: 0.33424283765347884\n",
      "--------------------------\n",
      "Logistic Regression\n",
      "--------------------------\n",
      "Cross validation 0.8657703081232493\n",
      "train_score: 0.8752436647173489\n",
      "test_score: 0.8363636363636363\n",
      "--------------------------\n",
      "Grid search: Decision Tree Classifier\n",
      "--------------------------\n",
      "Cross validation 0.8323809523809524\n",
      "train_score: 0.9044834307992202\n",
      "test_score: 0.8272727272727273\n"
     ]
    }
   ],
   "source": [
    "sf_per_day = sanfran.groupby([sanfran[\"start_date\"].dt.date]).mean()\n",
    "sf_per_day[\"count\"] = sanfran.groupby([sanfran[\"start_date\"].dt.date]).count()[\"mean_temperature_c\"]\n",
    "del sf_per_day['mean_humidity_binary']\n",
    "q1 = sf_per_day['count'].quantile(q=1/3)\n",
    "q2 = sf_per_day['count'].quantile(q=2/3)\n",
    "q3 = sf_per_day['count'].quantile(q=3/3)\n",
    "list_bin = []\n",
    "for val in sf_per_day['count']:\n",
    "    if val < q1:\n",
    "        list_bin.append(0)\n",
    "    elif val>= q1 and val < q2:\n",
    "        list_bin.append(1)\n",
    "    else:\n",
    "        list_bin.append(2)\n",
    "        \n",
    "sf_per_day['count_class'] = list_bin\n",
    "sf_per_day.dropna(inplace=True)\n",
    "del sf_per_day['count']\n",
    "\n",
    "X = sf_per_day.copy()\n",
    "y = X.pop('count_class')\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe_class.fit(X_train, y_train)\n",
    "print('SAN FRANCISCO:')\n",
    "print('--------------------------')\n",
    "print('Baseline:',sf_per_day.count_class.value_counts(normalize=True).max())\n",
    "print('--------------------------')\n",
    "print('Logistic Regression')\n",
    "print('--------------------------')\n",
    "print('Cross validation' , cross_val_score(pipe, X_train, y_train, cv=5).mean())\n",
    "print('train_score:',pipe.score(X_train, y_train))\n",
    "print('test_score:',pipe.score(X_test, y_test))\n",
    "print('--------------------------')\n",
    "print('Grid search: Decision Tree Classifier')\n",
    "print('--------------------------')\n",
    "print('Cross validation' , cross_val_score(pipe_class, X_train, y_train, cv=5).mean())\n",
    "print('train_score:',pipe_class.score(X_train, y_train))\n",
    "print('test_score:',pipe_class.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA:\n",
      "--------------------------\n",
      "Baseline: 0.33424283765347884\n",
      "--------------------------\n",
      "Logistic Regression\n",
      "--------------------------\n",
      "Cross validation 0.5328685136996786\n",
      "train_score: 0.5669515669515669\n",
      "test_score: 0.5980066445182725\n",
      "--------------------------\n",
      "Grid search: Decision Tree Classifier\n",
      "--------------------------\n",
      "Cross validation 0.6495476445591247\n",
      "train_score: 0.7450142450142451\n",
      "test_score: 0.7242524916943521\n"
     ]
    }
   ],
   "source": [
    "LA_per_day = LA.groupby([LA[\"start_time\"].dt.date]).mean()\n",
    "LA_per_day[\"count\"] = LA.groupby([LA[\"start_time\"].dt.date]).count()[\"bike_id\"]\n",
    "q1 = LA_per_day['count'].quantile(q=1/3)\n",
    "q2 = LA_per_day['count'].quantile(q=2/3)\n",
    "q3 = LA_per_day['count'].quantile(q=3/3)\n",
    "list_bin = []\n",
    "for val in LA_per_day['count']:\n",
    "    if val < q1:\n",
    "        list_bin.append(0)\n",
    "    elif val>= q1 and val <q2:\n",
    "        list_bin.append(1)\n",
    "    else:\n",
    "        list_bin.append(2)\n",
    "        \n",
    "LA_per_day['count_class'] = list_bin\n",
    "LA_per_day.dropna(inplace=True)\n",
    "del LA_per_day['count']\n",
    "\n",
    "X = LA_per_day.copy()\n",
    "y = X.pop('count_class')\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "print('LA:')\n",
    "print('--------------------------')\n",
    "print('Baseline:',sf_per_day.count_class.value_counts(normalize=True).max())\n",
    "print('--------------------------')\n",
    "print('Logistic Regression')\n",
    "print('--------------------------')\n",
    "print('Cross validation' , cross_val_score(pipe, X_train, y_train, cv=5).mean())\n",
    "print('train_score:',pipe.score(X_train, y_train))\n",
    "print('test_score:',pipe.score(X_test, y_test))\n",
    "print('--------------------------')\n",
    "\n",
    "classifier_1 = DecisionTreeClassifier(criterion='gini',\n",
    "                                    max_depth=3, \n",
    "                                    random_state=1)\n",
    "g_params_1 = {'max_depth':list(range(1, 11))+[None],'max_features':[1, 2],'max_leaf_nodes':[None,2,3,4],\n",
    "             'min_samples_leaf':[2,3,4, 5, 10, 15, 20, 25, 30, 40, 50],'min_samples_split':[2,3]}\n",
    "gridsearch_1 = GridSearchCV(classifier_1 , g_params_1 ,return_train_score=True,  cv=10)\n",
    "pipe_class_1 = Pipeline(steps=[('scaler', scaler),\n",
    "                       ('model', gridsearch_1)])\n",
    "pipe_class_1.fit(X_train, y_train)\n",
    "print('Grid search: Decision Tree Classifier')\n",
    "print('--------------------------')\n",
    "print('Cross validation' , cross_val_score(pipe_class_1, X_train, y_train, cv=5).mean())\n",
    "print('train_score:',pipe_class_1.score(X_train, y_train))\n",
    "print('test_score:',pipe_class_1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHICAGO:\n",
      "--------------------------\n",
      "Baseline: 0.33424283765347884\n",
      "--------------------------\n",
      "Logistic Regression\n",
      "--------------------------\n",
      "Cross validation 0.6431372549019608\n",
      "train_score: 0.6666666666666666\n",
      "test_score: 0.7181818181818181\n",
      "--------------------------\n",
      "Grid search: Decision Tree Classifier\n",
      "--------------------------\n",
      "Cross validation 0.7176470588235294\n",
      "train_score: 0.792156862745098\n",
      "test_score: 0.7636363636363637\n"
     ]
    }
   ],
   "source": [
    "chicago_per_day = chicago.groupby([chicago[\"start_time\"].dt.date]).mean()\n",
    "chicago_per_day[\"count\"] = chicago.groupby([chicago[\"start_time\"].dt.date]).count()[\"trip_id\"]\n",
    "q1 = chicago_per_day['count'].quantile(q=1/3)\n",
    "q2 = chicago_per_day['count'].quantile(q=2/3)\n",
    "q3 = chicago_per_day['count'].quantile(q=3/3)\n",
    "list_bin = []\n",
    "for val in chicago_per_day['count']:\n",
    "    if val < q1:\n",
    "        list_bin.append(0)\n",
    "    elif val>= q1 and val <q2:\n",
    "        list_bin.append(1)\n",
    "    else:\n",
    "        list_bin.append(2)\n",
    "        \n",
    "chicago_per_day['count_class'] = list_bin\n",
    "chicago_per_day.dropna(inplace=True)\n",
    "del chicago_per_day['count']\n",
    "\n",
    "X = chicago_per_day.copy()\n",
    "y = X.pop('count_class')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe_class.fit(X_train, y_train)\n",
    "print('CHICAGO:')\n",
    "print('--------------------------')\n",
    "print('Baseline:',sf_per_day.count_class.value_counts(normalize=True).max())\n",
    "print('--------------------------')\n",
    "print('Logistic Regression')\n",
    "print('--------------------------')\n",
    "print('Cross validation' , cross_val_score(pipe, X_train, y_train, cv=5).mean())\n",
    "print('train_score:',pipe.score(X_train, y_train))\n",
    "print('test_score:',pipe.score(X_test, y_test))\n",
    "print('--------------------------')\n",
    "print('Grid search: Decision Tree Classifier')\n",
    "print('--------------------------')\n",
    "print('Cross validation' , cross_val_score(pipe_class, X_train, y_train, cv=5).mean())\n",
    "print('train_score:',pipe_class.score(X_train, y_train))\n",
    "print('test_score:',pipe_class.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LONDON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONDON:\n",
      "--------------------------\n",
      "Baseline: 0.33424657534246577\n",
      "--------------------------\n",
      "Logistic Regression\n",
      "--------------------------\n",
      "Cross validation 0.7455549209975253\n",
      "train_score: 0.7710371819960861\n",
      "test_score: 0.7808219178082192\n",
      "--------------------------\n",
      "Grid search: Decision Tree Classifier\n",
      "--------------------------\n",
      "Cross validation 0.6672758423757853\n",
      "train_score: 0.7729941291585127\n",
      "test_score: 0.6894977168949772\n"
     ]
    }
   ],
   "source": [
    "london_per_day = london.groupby([london[\"timestamp\"].dt.date]).mean()\n",
    "london_per_day.drop(['is_weekend','hum_binary','t2'],axis=1,inplace=True)\n",
    "q1 = london_per_day['cnt'].quantile(q=1/3)\n",
    "q2 = london_per_day['cnt'].quantile(q=2/3)\n",
    "q3 = london_per_day['cnt'].quantile(q=3/3)\n",
    "list_bin = []\n",
    "for val in london_per_day['cnt']:\n",
    "    if val < q1:\n",
    "        list_bin.append(0)\n",
    "    elif val>= q1 and val <q2:\n",
    "        list_bin.append(1)\n",
    "    else:\n",
    "        list_bin.append(2)\n",
    "        \n",
    "london_per_day['count_class'] = list_bin\n",
    "del london_per_day['cnt']\n",
    "\n",
    "X = london_per_day.copy()\n",
    "y = X.pop('count_class')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe_class.fit(X_train, y_train)\n",
    "print('LONDON:')\n",
    "print('--------------------------')\n",
    "print('Baseline:',london_per_day.count_class.value_counts(normalize=True).max())\n",
    "print('--------------------------')\n",
    "print('Logistic Regression')\n",
    "print('--------------------------')\n",
    "print('Cross validation' , cross_val_score(pipe, X_train, y_train, cv=5).mean())\n",
    "print('train_score:',pipe.score(X_train, y_train))\n",
    "print('test_score:',pipe.score(X_test, y_test))\n",
    "print('--------------------------')\n",
    "print('Grid search: Decision Tree Classifier')\n",
    "print('--------------------------')\n",
    "print('Cross validation' , cross_val_score(pipe_class, X_train, y_train, cv=5).mean())\n",
    "print('train_score:',pipe_class.score(X_train, y_train))\n",
    "print('test_score:',pipe_class.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC:\n",
      "--------------------------\n",
      "Baseline: 0.3333333333333333\n",
      "--------------------------\n",
      "Logistic Regression\n",
      "--------------------------\n",
      "Cross validation 0.9306412337662338\n",
      "train_score: 0.9623824451410659\n",
      "test_score: 0.927007299270073\n",
      "--------------------------\n",
      "Grid search: Decision Tree Classifier\n",
      "--------------------------\n",
      "Cross validation 0.8963564213564214\n",
      "train_score: 0.9843260188087775\n",
      "test_score: 0.8978102189781022\n"
     ]
    }
   ],
   "source": [
    "dc_per_day = DC.groupby([DC[\"datetime\"].dt.date]).mean()\n",
    "dc_per_day[\"count\"] = (DC[\"count\"].groupby([DC[\"datetime\"].dt.date]).sum())\n",
    "q1 = dc_per_day['count'].quantile(q=1/3)\n",
    "q2 = dc_per_day['count'].quantile(q=2/3)\n",
    "q3 = dc_per_day['count'].quantile(q=3/3)\n",
    "list_bin = []\n",
    "for val in dc_per_day['count']:\n",
    "    if val < q1:\n",
    "        list_bin.append(0)\n",
    "    elif val>= q1 and val < q2:\n",
    "        list_bin.append(1)\n",
    "    else:\n",
    "        list_bin.append(2)\n",
    "                \n",
    "dc_per_day['count_class'] = list_bin\n",
    "del dc_per_day['count']\n",
    "\n",
    "\n",
    "X = dc_per_day.copy()\n",
    "y = X.pop('count_class')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=1)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe_class.fit(X_train, y_train)\n",
    "print('DC:')\n",
    "print('--------------------------')\n",
    "print('Baseline:',dc_per_day.count_class.value_counts(normalize=True).max())\n",
    "print('--------------------------')\n",
    "print('Logistic Regression')\n",
    "print('--------------------------')\n",
    "print('Cross validation' , cross_val_score(pipe, X_train, y_train, cv=5).mean())\n",
    "print('train_score:',pipe.score(X_train, y_train))\n",
    "print('test_score:',pipe.score(X_test, y_test))\n",
    "print('--------------------------')\n",
    "print('Grid search: Decision Tree Classifier')\n",
    "print('--------------------------')\n",
    "print('Cross validation' , cross_val_score(pipe_class, X_train, y_train, cv=5).mean())\n",
    "print('train_score:',pipe_class.score(X_train, y_train))\n",
    "print('test_score:',pipe_class.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 0 0 2 2 1 2 2 2 1 0 1 0 2 0 1 1 0 1 0 2 2 2 1 0 2 0 1 2 2 1 2 1 0 0\n",
      " 0 0 1 1 2 1 1 1 0 0 1 1 1 1 2 0 0 1 0 1 1 2 0 1 2 1 2 2 1 2 0 1 1 1 0 2 2\n",
      " 0 0 2 0 0 1 1 1 2 0 2 1 1 0 1 1 0 0 2 0 1 2 0 1 2 2 1 1 1 2 2 2 2 1 0 0 1\n",
      " 2 1 1 0 2 0 1 2 2 2 2 0 2 2 0 0 2 0 0 2 0 1 0 2 1 0]\n",
      "[0 1 2 0 0 2 2 1 1 2 2 1 0 1 0 2 0 1 1 0 1 0 2 2 2 2 0 1 0 1 2 2 1 2 2 0 0\n",
      " 0 0 1 1 2 1 1 1 0 0 1 1 1 0 2 0 0 1 1 1 1 2 0 1 2 1 2 1 1 1 0 1 1 0 0 2 2\n",
      " 0 0 2 0 0 1 1 1 2 0 2 1 1 0 1 1 0 0 2 0 1 2 0 1 1 2 1 1 1 2 1 2 2 1 0 0 1\n",
      " 2 1 1 0 2 0 1 2 2 2 2 0 2 1 0 0 2 0 0 2 0 1 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(pipe.predict(X_test))\n",
    "print(pipe_class.predict(X_test))\n",
    "#in a dataframe > value counts or uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_class.named_steps['model'].best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(pipe_class.named_steps['model'].best_estimator_.feature_importances_,\n",
    "                                   index=X.columns,\n",
    "                                   columns=['importance'])\n",
    "\n",
    "feature_importances.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_class.named_steps['model'].best_estimator_.feature_importances_, X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(pipe_class.best_estimator_,\n",
    "                out_file=dot_data,\n",
    "                filled=True,\n",
    "                rounded=True,\n",
    "                special_characters=True,\n",
    "                class_names=True\n",
    "                )\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "Image(graph.create_png())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
